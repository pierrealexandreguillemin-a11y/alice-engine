{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALICE Engine - ML Training Pipeline\n",
    "\n",
    "**Objectif:** Prédiction des résultats d'échecs (Interclubs FFE)\n",
    "\n",
    "## Conformité ISO\n",
    "\n",
    "| Norme | Description | Statut |\n",
    "|-------|-------------|--------|\n",
    "| ISO/IEC 5259 | Data Quality for ML | Validé |\n",
    "| ISO/IEC 42001 | AI Management System (Model Card) | Validé |\n",
    "| ISO/IEC 24029 | Neural Network Robustness | Testé |\n",
    "| ISO/IEC 24027 | Bias in AI (Fairness) | Testé |\n",
    "| ISO/IEC 25059 | AI Quality Model | Rapport final |\n",
    "\n",
    "## Métadonnées\n",
    "\n",
    "- **Version:** 2.0.0\n",
    "- **Auteur:** ALICE Engine Team\n",
    "- **Dataset:** FFE Interclubs 2014-2024\n",
    "- **Target:** Victoire Blancs (binaire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Configuration & Seeds (Reproducibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration globale - Reproducibilité\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Paramètres d'entraînement\n",
    "CONFIG = {\n",
    "    'seed': RANDOM_SEED,\n",
    "    'target_column': 'target',\n",
    "    'eval_metric': 'roc_auc',\n",
    "    'threshold_auc': 0.70,  # ISO 25059 minimum\n",
    "    'autogluon_time_limit': 21600,  # 6 heures\n",
    "    'autogluon_presets': 'best_quality',  # Kaggle: best_quality (extreme trop long)\n",
    "    'num_bag_folds': 5,\n",
    "    'num_stack_levels': 2,\n",
    "}\n",
    "\n",
    "# Features\n",
    "FEATURES = [\n",
    "    'blanc_elo', 'noir_elo', 'diff_elo', 'echiquier', 'niveau', 'ronde',\n",
    "    'type_competition', 'division', 'ligue_code', 'blanc_titre', 'noir_titre', 'jour_semaine'\n",
    "]\n",
    "\n",
    "CAT_FEATURES = ['type_competition', 'division', 'ligue_code', 'blanc_titre', 'noir_titre', 'jour_semaine']\n",
    "\n",
    "print(f'Config: seed={CONFIG[\"seed\"]}, metric={CONFIG[\"eval_metric\"]}, threshold={CONFIG[\"threshold_auc\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation (Kaggle)\n",
    "!pip install -q autogluon.tabular catboost xgboost lightgbm scikit-learn pandas pyarrow matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import gc\n",
    "import json\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical tests\n",
    "from scipy.stats import chi2\n",
    "\n",
    "print('Libraries imported successfully')\n",
    "print(f'Pandas: {pd.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Loading (ISO 5259 - Data Quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemins Kaggle\n",
    "DATA_PATH = Path('/kaggle/input/alice-features')\n",
    "OUTPUT_PATH = Path('/kaggle/working')\n",
    "\n",
    "# Chargement des données\n",
    "train = pd.read_parquet(DATA_PATH / 'train.parquet')\n",
    "valid = pd.read_parquet(DATA_PATH / 'valid.parquet')\n",
    "test = pd.read_parquet(DATA_PATH / 'test.parquet')\n",
    "\n",
    "print(f'Train: {len(train):,} samples')\n",
    "print(f'Valid: {len(valid):,} samples')\n",
    "print(f'Test:  {len(test):,} samples')\n",
    "print(f'Total: {len(train) + len(valid) + len(test):,} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISO 5259: Validation intégrité données (hash)\n",
    "def compute_hash(df: pd.DataFrame) -> str:\n",
    "    \"\"\"Compute SHA256 hash for data lineage.\"\"\"\n",
    "    return hashlib.sha256(pd.util.hash_pandas_object(df).values.tobytes()).hexdigest()[:8]\n",
    "\n",
    "data_lineage = {\n",
    "    'train': {'samples': len(train), 'hash': compute_hash(train)},\n",
    "    'valid': {'samples': len(valid), 'hash': compute_hash(valid)},\n",
    "    'test': {'samples': len(test), 'hash': compute_hash(test)},\n",
    "}\n",
    "\n",
    "print('ISO 5259 Data Lineage:')\n",
    "for name, info in data_lineage.items():\n",
    "    print(f'  {name}: {info[\"samples\"]:,} samples, hash={info[\"hash\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Data Exploration (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aperçu des données\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques descriptives\n",
    "train[FEATURES].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution target\n",
    "for df in [train, valid, test]:\n",
    "    df['target'] = (df['resultat_blanc'] == 1.0).astype(int)\n",
    "\n",
    "print('Target distribution:')\n",
    "print(f'  Train: {train[\"target\"].mean():.2%} positive')\n",
    "print(f'  Valid: {valid[\"target\"].mean():.2%} positive')\n",
    "print(f'  Test:  {test[\"target\"].mean():.2%} positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation: Distribution ELO\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].hist(train['blanc_elo'], bins=50, alpha=0.7, label='Blancs')\n",
    "axes[0].hist(train['noir_elo'], bins=50, alpha=0.7, label='Noirs')\n",
    "axes[0].set_xlabel('ELO')\n",
    "axes[0].set_ylabel('Fréquence')\n",
    "axes[0].set_title('Distribution ELO')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(train['diff_elo'], bins=50, color='green', alpha=0.7)\n",
    "axes[1].set_xlabel('Différence ELO (Blanc - Noir)')\n",
    "axes[1].set_title('Distribution Diff ELO')\n",
    "\n",
    "train['target'].value_counts().plot(kind='bar', ax=axes[2], color=['red', 'green'])\n",
    "axes[2].set_xlabel('Target (0=Défaite/Nulle, 1=Victoire Blanc)')\n",
    "axes[2].set_title('Distribution Target')\n",
    "axes[2].set_xticklabels(['0', '1'], rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / 'eda_distributions.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des datasets\n",
    "combined = pd.concat([train, valid], ignore_index=True)\n",
    "print(f'Combined train+valid: {len(combined):,} samples')\n",
    "\n",
    "X_train = combined[FEATURES]\n",
    "y_train = combined['target']\n",
    "X_test = test[FEATURES]\n",
    "y_test = test['target']\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodage pour XGBoost/LightGBM (CatBoost gère nativement)\n",
    "X_train_encoded = X_train.copy()\n",
    "X_test_encoded = X_test.copy()\n",
    "\n",
    "label_encoders = {}\n",
    "for col in CAT_FEATURES:\n",
    "    le = LabelEncoder()\n",
    "    X_train_encoded[col] = le.fit_transform(X_train_encoded[col].astype(str))\n",
    "    X_test_encoded[col] = le.transform(X_test_encoded[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "print('Label encoding completed for XGBoost/LightGBM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Baseline Models (CatBoost, XGBoost, LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stockage des résultats\n",
    "results = {}\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# CatBoost\n",
    "print('Training CatBoost...')\n",
    "cb = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.03,\n",
    "    depth=6,\n",
    "    cat_features=CAT_FEATURES,\n",
    "    early_stopping_rounds=50,\n",
    "    random_seed=RANDOM_SEED,\n",
    "    verbose=100\n",
    ")\n",
    "cb.fit(X_train, y_train, eval_set=(X_test, y_test))\n",
    "\n",
    "results['CatBoost'] = roc_auc_score(y_test, cb.predict_proba(X_test)[:, 1])\n",
    "models['CatBoost'] = cb\n",
    "print(f'CatBoost Test AUC: {results[\"CatBoost\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# XGBoost\n",
    "print('Training XGBoost...')\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=6,\n",
    "    early_stopping_rounds=50,\n",
    "    random_state=RANDOM_SEED,\n",
    "    eval_metric='auc'\n",
    ")\n",
    "xgb.fit(X_train_encoded, y_train, eval_set=[(X_test_encoded, y_test)], verbose=100)\n",
    "\n",
    "results['XGBoost'] = roc_auc_score(y_test, xgb.predict_proba(X_test_encoded)[:, 1])\n",
    "models['XGBoost'] = xgb\n",
    "print(f'XGBoost Test AUC: {results[\"XGBoost\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# LightGBM\n",
    "print('Training LightGBM...')\n",
    "lgb = LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=63,\n",
    "    random_state=RANDOM_SEED,\n",
    "    verbose=100\n",
    ")\n",
    "lgb.fit(\n",
    "    X_train_encoded, y_train,\n",
    "    eval_set=[(X_test_encoded, y_test)],\n",
    "    eval_metric='auc',\n",
    "    callbacks=[lgb.early_stopping(50)]\n",
    ")\n",
    "\n",
    "results['LightGBM'] = roc_auc_score(y_test, lgb.predict_proba(X_test_encoded)[:, 1])\n",
    "models['LightGBM'] = lgb\n",
    "print(f'LightGBM Test AUC: {results[\"LightGBM\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Résumé Baseline\n",
    "print('\\n=== BASELINE RESULTS ===')\n",
    "for model_name, auc in sorted(results.items(), key=lambda x: -x[1]):\n",
    "    status = 'PASS' if auc >= CONFIG['threshold_auc'] else 'FAIL'\n",
    "    print(f'{model_name}: {auc:.4f} [{status}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. AutoGluon (Preset: best_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libération mémoire\n",
    "del combined\n",
    "gc.collect()\n",
    "\n",
    "# Préparation données AutoGluon\n",
    "train_ag = pd.concat([train[FEATURES + ['target']], valid[FEATURES + ['target']]], ignore_index=True)\n",
    "test_ag = test[FEATURES + ['target']]\n",
    "\n",
    "print(f'AutoGluon train: {len(train_ag):,}')\n",
    "print(f'AutoGluon test: {len(test_ag):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# AutoGluon Training\n",
    "print('Training AutoGluon (best_quality preset)...')\n",
    "print(f'Time limit: {CONFIG[\"autogluon_time_limit\"]} seconds ({CONFIG[\"autogluon_time_limit\"]/3600:.1f} hours)')\n",
    "\n",
    "predictor = TabularPredictor(\n",
    "    label=CONFIG['target_column'],\n",
    "    eval_metric=CONFIG['eval_metric'],\n",
    "    path=str(OUTPUT_PATH / 'autogluon')\n",
    ")\n",
    "\n",
    "predictor.fit(\n",
    "    train_data=train_ag,\n",
    "    presets=CONFIG['autogluon_presets'],\n",
    "    time_limit=CONFIG['autogluon_time_limit'],\n",
    "    num_bag_folds=CONFIG['num_bag_folds'],\n",
    "    num_stack_levels=CONFIG['num_stack_levels'],\n",
    "    verbosity=2,\n",
    "    ag_args_fit={'ag.max_memory_usage_ratio': 3.0}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaderboard AutoGluon\n",
    "leaderboard = predictor.leaderboard()\n",
    "print(f'\\nAutoGluon trained {len(leaderboard)} models')\n",
    "leaderboard[['model', 'score_val', 'fit_time']].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation sur test set\n",
    "ag_proba = predictor.predict_proba(test_ag.drop(columns='target'))\n",
    "results['AutoGluon'] = roc_auc_score(test_ag['target'], ag_proba[1])\n",
    "print(f'AutoGluon Test AUC: {results[\"AutoGluon\"]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. ISO 24029 - Robustness Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_robustness(predictor, test_data: pd.DataFrame, noise_level: float = 0.1) -> dict:\n",
    "    \"\"\"ISO 24029: Test model robustness to input perturbations.\"\"\"\n",
    "    X_test = test_data.drop(columns='target')\n",
    "    y_test = test_data['target']\n",
    "    \n",
    "    # Baseline AUC\n",
    "    baseline_auc = roc_auc_score(y_test, predictor.predict_proba(X_test)[1])\n",
    "    \n",
    "    # Perturbed AUC (add Gaussian noise to numeric features)\n",
    "    X_noisy = X_test.copy()\n",
    "    numeric_cols = X_noisy.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        std = X_noisy[col].std()\n",
    "        X_noisy[col] = X_noisy[col] + np.random.normal(0, std * noise_level, len(X_noisy))\n",
    "    \n",
    "    noisy_auc = roc_auc_score(y_test, predictor.predict_proba(X_noisy)[1])\n",
    "    \n",
    "    # Tolerance calculation\n",
    "    tolerance = baseline_auc - noisy_auc\n",
    "    \n",
    "    # Status based on tolerance\n",
    "    if tolerance < 0.02:\n",
    "        status = 'ROBUST'\n",
    "    elif tolerance < 0.05:\n",
    "        status = 'ACCEPTABLE'\n",
    "    else:\n",
    "        status = 'FRAGILE'\n",
    "    \n",
    "    return {\n",
    "        'baseline_auc': float(baseline_auc),\n",
    "        'noisy_auc': float(noisy_auc),\n",
    "        'noise_tolerance': float(tolerance),\n",
    "        'noise_level': noise_level,\n",
    "        'status': status\n",
    "    }\n",
    "\n",
    "robustness_report = test_robustness(predictor, test_ag, noise_level=0.1)\n",
    "print('ISO 24029 Robustness Report:')\n",
    "print(f'  Baseline AUC: {robustness_report[\"baseline_auc\"]:.4f}')\n",
    "print(f'  Noisy AUC: {robustness_report[\"noisy_auc\"]:.4f}')\n",
    "print(f'  Tolerance: {robustness_report[\"noise_tolerance\"]:.4f}')\n",
    "print(f'  Status: {robustness_report[\"status\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. ISO 24027 - Fairness Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fairness(predictor, test_data: pd.DataFrame, sensitive_attr: str) -> dict:\n",
    "    \"\"\"ISO 24027: Test model fairness across groups.\"\"\"\n",
    "    X_test = test_data.drop(columns='target')\n",
    "    y_test = test_data['target']\n",
    "    y_pred = predictor.predict(X_test)\n",
    "    \n",
    "    # Positive rate per group\n",
    "    groups = test_data[sensitive_attr].unique()\n",
    "    positive_rates = {}\n",
    "    \n",
    "    for group in groups:\n",
    "        mask = test_data[sensitive_attr] == group\n",
    "        if mask.sum() > 100:  # Minimum sample size\n",
    "            positive_rates[str(group)] = float(y_pred[mask].mean())\n",
    "    \n",
    "    # Demographic parity (max difference between groups)\n",
    "    rates = list(positive_rates.values())\n",
    "    demographic_parity = max(rates) - min(rates) if rates else 0\n",
    "    \n",
    "    # Status\n",
    "    if demographic_parity < 0.05:\n",
    "        status = 'FAIR'\n",
    "    elif demographic_parity < 0.10:\n",
    "        status = 'ACCEPTABLE'\n",
    "    else:\n",
    "        status = 'CRITICAL'\n",
    "    \n",
    "    return {\n",
    "        'sensitive_attribute': sensitive_attr,\n",
    "        'positive_rates_by_group': positive_rates,\n",
    "        'demographic_parity': float(demographic_parity),\n",
    "        'status': status\n",
    "    }\n",
    "\n",
    "fairness_report = test_fairness(predictor, test_ag, 'ligue_code')\n",
    "print('ISO 24027 Fairness Report:')\n",
    "print(f'  Sensitive attribute: {fairness_report[\"sensitive_attribute\"]}')\n",
    "print(f'  Demographic parity: {fairness_report[\"demographic_parity\"]:.4f}')\n",
    "print(f'  Status: {fairness_report[\"status\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. McNemar Statistical Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcnemar_test(y_true, pred_a, pred_b):\n",
    "    \"\"\"McNemar test for comparing two classifiers.\"\"\"\n",
    "    correct_a = (pred_a == y_true)\n",
    "    correct_b = (pred_b == y_true)\n",
    "    \n",
    "    # Contingency: b correct & a wrong, a correct & b wrong\n",
    "    b = np.sum(correct_a & ~correct_b)\n",
    "    c = np.sum(~correct_a & correct_b)\n",
    "    \n",
    "    if b + c == 0:\n",
    "        return 0.0, 1.0\n",
    "    \n",
    "    statistic = (abs(b - c) - 1) ** 2 / (b + c)\n",
    "    p_value = 1 - chi2.cdf(statistic, 1)\n",
    "    \n",
    "    return statistic, p_value\n",
    "\n",
    "# Comparaison AutoGluon vs Baseline (CatBoost)\n",
    "y_true = test_ag['target'].values\n",
    "pred_ag = predictor.predict(test_ag.drop(columns='target')).values\n",
    "pred_cb = models['CatBoost'].predict(X_test).astype(int)\n",
    "\n",
    "stat, p_value = mcnemar_test(y_true, pred_ag, pred_cb)\n",
    "\n",
    "baseline_best_auc = max(results['CatBoost'], results['XGBoost'], results['LightGBM'])\n",
    "diff_pct = (results['AutoGluon'] - baseline_best_auc) * 100\n",
    "\n",
    "mcnemar_report = {\n",
    "    'autogluon_auc': float(results['AutoGluon']),\n",
    "    'baseline_best_auc': float(baseline_best_auc),\n",
    "    'difference_pct': float(diff_pct),\n",
    "    'mcnemar_statistic': float(stat),\n",
    "    'p_value': float(p_value),\n",
    "    'significant': p_value < 0.05,\n",
    "    'meets_2pct': diff_pct >= 2.0\n",
    "}\n",
    "\n",
    "# Décision\n",
    "if mcnemar_report['significant'] and mcnemar_report['meets_2pct']:\n",
    "    mcnemar_report['winner'] = 'AutoGluon'\n",
    "else:\n",
    "    mcnemar_report['winner'] = 'Baseline'\n",
    "\n",
    "print('McNemar Comparison:')\n",
    "print(f'  AutoGluon AUC: {mcnemar_report[\"autogluon_auc\"]:.4f}')\n",
    "print(f'  Baseline Best AUC: {mcnemar_report[\"baseline_best_auc\"]:.4f}')\n",
    "print(f'  Difference: {mcnemar_report[\"difference_pct\"]:+.2f}%')\n",
    "print(f'  p-value: {mcnemar_report[\"p_value\"]:.4f}')\n",
    "print(f'  Significant (p<0.05): {mcnemar_report[\"significant\"]}')\n",
    "print(f'  Winner: {mcnemar_report[\"winner\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. ISO 42001 - Model Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISO 42001 Model Card\n",
    "model_card = {\n",
    "    'model_id': f'autogluon_{datetime.now():%Y%m%d_%H%M%S}',\n",
    "    'model_name': 'AutoGluon_best_quality',\n",
    "    'version': '2.0.0',\n",
    "    'created_at': datetime.now().isoformat(),\n",
    "    \n",
    "    # Purpose\n",
    "    'purpose': 'Prediction of chess game outcomes (White wins) for FFE Interclubs',\n",
    "    'intended_use': 'Team composition optimization, player performance analysis',\n",
    "    \n",
    "    # Data\n",
    "    'training_data': {\n",
    "        'source': 'FFE Interclubs 2014-2024',\n",
    "        'samples': len(train_ag),\n",
    "        'hash': compute_hash(train_ag),\n",
    "        'features': FEATURES\n",
    "    },\n",
    "    'test_data': {\n",
    "        'samples': len(test_ag),\n",
    "        'hash': compute_hash(test_ag)\n",
    "    },\n",
    "    \n",
    "    # Hyperparameters\n",
    "    'hyperparameters': {\n",
    "        'presets': CONFIG['autogluon_presets'],\n",
    "        'time_limit': CONFIG['autogluon_time_limit'],\n",
    "        'num_bag_folds': CONFIG['num_bag_folds'],\n",
    "        'num_stack_levels': CONFIG['num_stack_levels']\n",
    "    },\n",
    "    \n",
    "    # Performance\n",
    "    'performance': {\n",
    "        'test_auc': results['AutoGluon'],\n",
    "        'best_model': leaderboard.iloc[0]['model'],\n",
    "        'num_models': len(leaderboard)\n",
    "    },\n",
    "    \n",
    "    # ISO Compliance\n",
    "    'iso_compliance': {\n",
    "        'iso_24029_robustness': robustness_report['status'],\n",
    "        'iso_24027_fairness': fairness_report['status'],\n",
    "        'iso_25059_threshold': results['AutoGluon'] >= CONFIG['threshold_auc']\n",
    "    },\n",
    "    \n",
    "    # Limitations\n",
    "    'limitations': [\n",
    "        'Only applicable to French chess federation data',\n",
    "        'Performance may vary for different competition types',\n",
    "        'Does not account for player form/fatigue'\n",
    "    ],\n",
    "    \n",
    "    # Contact\n",
    "    'contact': 'ALICE Engine Team'\n",
    "}\n",
    "\n",
    "# Save Model Card\n",
    "with open(OUTPUT_PATH / 'model_card.json', 'w') as f:\n",
    "    json.dump(model_card, f, indent=2, default=str)\n",
    "\n",
    "print('ISO 42001 Model Card saved')\n",
    "print(f'  Model ID: {model_card[\"model_id\"]}')\n",
    "print(f'  Test AUC: {model_card[\"performance\"][\"test_auc\"]:.4f}')\n",
    "print(f'  Best model: {model_card[\"performance\"][\"best_model\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des modèles\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "model_names = list(results.keys())\n",
    "aucs = list(results.values())\n",
    "colors = ['#4CAF50' if auc >= CONFIG['threshold_auc'] else '#F44336' for auc in aucs]\n",
    "\n",
    "bars = ax.barh(model_names, aucs, color=colors)\n",
    "ax.axvline(x=CONFIG['threshold_auc'], color='red', linestyle='--', linewidth=2, label=f'ISO 25059 Threshold ({CONFIG[\"threshold_auc\"]})')\n",
    "\n",
    "for bar, auc in zip(bars, aucs):\n",
    "    ax.text(auc + 0.005, bar.get_y() + bar.get_height()/2, f'{auc:.4f}', va='center', fontsize=12)\n",
    "\n",
    "ax.set_xlabel('Test AUC', fontsize=12)\n",
    "ax.set_title('ALICE Engine - Model Comparison', fontsize=14)\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_xlim(0.5, max(aucs) + 0.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / 'model_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance (CatBoost)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "importance = pd.DataFrame({\n",
    "    'feature': FEATURES,\n",
    "    'importance': models['CatBoost'].feature_importances_\n",
    "}).sort_values('importance', ascending=True)\n",
    "\n",
    "ax.barh(importance['feature'], importance['importance'], color='steelblue')\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_title('Feature Importance (CatBoost)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / 'feature_importance.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. ISO 25059 Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération rapport final\n",
    "report = f'''# Rapport ISO 25059 - ALICE Engine ML Training\n",
    "\n",
    "**Date:** {datetime.now():%Y-%m-%d %H:%M}\n",
    "**Version:** 2.0.0\n",
    "\n",
    "---\n",
    "\n",
    "## Résumé Exécutif\n",
    "\n",
    "| Critère | Résultat | Statut |\n",
    "|---------|----------|--------|\n",
    "| AutoGluon AUC | {results[\"AutoGluon\"]:.4f} | {\"PASS\" if results[\"AutoGluon\"] >= CONFIG[\"threshold_auc\"] else \"FAIL\"} |\n",
    "| ISO 24029 Robustesse | {robustness_report[\"status\"]} | {\"PASS\" if robustness_report[\"status\"] != \"FRAGILE\" else \"FAIL\"} |\n",
    "| ISO 24027 Fairness | {fairness_report[\"status\"]} | {\"PASS\" if fairness_report[\"status\"] != \"CRITICAL\" else \"FAIL\"} |\n",
    "| Diff vs Baseline | {mcnemar_report[\"difference_pct\"]:+.2f}% | {\"PASS\" if mcnemar_report[\"meets_2pct\"] else \"WARN\"} |\n",
    "| p-value (McNemar) | {mcnemar_report[\"p_value\"]:.4f} | {\"PASS\" if mcnemar_report[\"significant\"] else \"FAIL\"} |\n",
    "\n",
    "**Recommandation:** {mcnemar_report[\"winner\"]}\n",
    "\n",
    "---\n",
    "\n",
    "## Baseline Models\n",
    "\n",
    "| Model | Test AUC | Status |\n",
    "|-------|----------|--------|\n",
    "| CatBoost | {results[\"CatBoost\"]:.4f} | {\"PASS\" if results[\"CatBoost\"] >= CONFIG[\"threshold_auc\"] else \"FAIL\"} |\n",
    "| XGBoost | {results[\"XGBoost\"]:.4f} | {\"PASS\" if results[\"XGBoost\"] >= CONFIG[\"threshold_auc\"] else \"FAIL\"} |\n",
    "| LightGBM | {results[\"LightGBM\"]:.4f} | {\"PASS\" if results[\"LightGBM\"] >= CONFIG[\"threshold_auc\"] else \"FAIL\"} |\n",
    "\n",
    "---\n",
    "\n",
    "## AutoGluon Results\n",
    "\n",
    "- **Preset:** {CONFIG[\"autogluon_presets\"]}\n",
    "- **Time Limit:** {CONFIG[\"autogluon_time_limit\"]/3600:.1f} hours\n",
    "- **Models Trained:** {len(leaderboard)}\n",
    "- **Best Model:** {leaderboard.iloc[0][\"model\"]}\n",
    "- **Test AUC:** {results[\"AutoGluon\"]:.4f}\n",
    "\n",
    "---\n",
    "\n",
    "## Décision Finale\n",
    "\n",
    "**Règle:**\n",
    "```\n",
    "IF AutoGluon.AUC >= 0.70\n",
    "   AND robustness != FRAGILE\n",
    "   AND fairness != CRITICAL\n",
    "   AND p_value < 0.05\n",
    "   AND diff >= +2%\n",
    "THEN AutoGluon\n",
    "ELSE Baseline (CatBoost)\n",
    "```\n",
    "\n",
    "**Résultat:** {mcnemar_report[\"winner\"]}\n",
    "\n",
    "---\n",
    "\n",
    "## Data Lineage (ISO 5259)\n",
    "\n",
    "| Dataset | Samples | Hash |\n",
    "|---------|---------|------|\n",
    "| Train | {data_lineage[\"train\"][\"samples\"]:,} | {data_lineage[\"train\"][\"hash\"]} |\n",
    "| Valid | {data_lineage[\"valid\"][\"samples\"]:,} | {data_lineage[\"valid\"][\"hash\"]} |\n",
    "| Test | {data_lineage[\"test\"][\"samples\"]:,} | {data_lineage[\"test\"][\"hash\"]} |\n",
    "\n",
    "---\n",
    "\n",
    "*Generated by ALICE Engine Kaggle Notebook*\n",
    "'''\n",
    "\n",
    "with open(OUTPUT_PATH / 'ISO_25059_REPORT.md', 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print('ISO 25059 Report saved to:', OUTPUT_PATH / 'ISO_25059_REPORT.md')\n",
    "print('\\n' + '='*50)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 14. Save All Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde de tous les rapports JSON\n",
    "reports_to_save = {\n",
    "    'autogluon_results.json': {\n",
    "        'test_auc': results['AutoGluon'],\n",
    "        'best_model': leaderboard.iloc[0]['model'],\n",
    "        'num_models': len(leaderboard),\n",
    "        'leaderboard': leaderboard[['model', 'score_val']].to_dict('records')\n",
    "    },\n",
    "    'robustness_report.json': robustness_report,\n",
    "    'fairness_report.json': fairness_report,\n",
    "    'mcnemar_comparison.json': mcnemar_report,\n",
    "    'baseline_results.json': {\n",
    "        'CatBoost': {'test_auc': results['CatBoost']},\n",
    "        'XGBoost': {'test_auc': results['XGBoost']},\n",
    "        'LightGBM': {'test_auc': results['LightGBM']}\n",
    "    }\n",
    "}\n",
    "\n",
    "for filename, data in reports_to_save.items():\n",
    "    with open(OUTPUT_PATH / filename, 'w') as f:\n",
    "        json.dump(data, f, indent=2, default=str)\n",
    "    print(f'Saved: {filename}')\n",
    "\n",
    "print('\\nAll reports saved successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des fichiers générés\n",
    "print('\\n=== OUTPUT FILES ===')\n",
    "for f in sorted(OUTPUT_PATH.glob('*')):\n",
    "    if f.is_file():\n",
    "        print(f'  {f.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Ce notebook a exécuté un pipeline ML complet conforme aux normes ISO:\n",
    "\n",
    "- **ISO 5259**: Data quality avec lineage et hash\n",
    "- **ISO 42001**: Model Card documenté\n",
    "- **ISO 24029**: Tests de robustesse\n",
    "- **ISO 24027**: Tests de fairness\n",
    "- **ISO 25059**: Rapport qualité final\n",
    "\n",
    "Pour reproduire les résultats localement, consultez le dépôt ALICE Engine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
